{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import os\n",
    "import gzip\n",
    "import io\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from operator import itemgetter\n",
    "from ts2vg import NaturalVG\n",
    "from ts2vg import HorizontalVG\n",
    "from math import sqrt\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Google :\n",
    "\n",
    "    def __init__(self, cred_file_path, version, service_name, scopes) -> None:\n",
    "        self.SCOPES = scopes\n",
    "        self.cred_file_path = cred_file_path\n",
    "        self.version = version\n",
    "        self.service_name = service_name\n",
    "\n",
    "    def connect(self) :\n",
    "        \"\"\"Makes a connection to the drive API and returns a service object\n",
    "        \"\"\"\n",
    "        creds = None\n",
    "        # The file token.json stores the user's access and refresh tokens, and is\n",
    "        # created automatically when the authorization flow completes for the first\n",
    "        # time.\n",
    "        if os.path.exists('token.json'):\n",
    "            creds = Credentials.from_authorized_user_file('token.json', self.SCOPES)\n",
    "        # If there are no (valid) credentials available, let the user log in.\n",
    "        if not creds or not creds.valid:\n",
    "            if creds and creds.expired and creds.refresh_token:\n",
    "                creds.refresh(Request())\n",
    "            else:\n",
    "                flow = InstalledAppFlow.from_client_secrets_file(self.cred_file_path, self.SCOPES)\n",
    "                creds = flow.run_local_server(port=0)\n",
    "            # Save the credentials for the next run\n",
    "            with open('token.json', 'w') as token:\n",
    "                token.write(creds.to_json())\n",
    "\n",
    "        try:\n",
    "            service = build(self.service_name, self.version, credentials=creds)\n",
    "            return service\n",
    "\n",
    "        except HttpError as error:\n",
    "            # TODO(developer) - Handle errors from drive API.\n",
    "            print(f'An error occurred: {error}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "connection_class = Google(os.getenv('credentials_file_path'), 'v3', 'drive', ['https://www.googleapis.com/auth/drive'])\n",
    "service = connection_class.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_id = os.getenv('folder_id')\n",
    "query = f\"'{folder_id}' in parents\"\n",
    "\n",
    "res = service.files().list(q = query).execute()\n",
    "files = res.get('files')\n",
    "next_page_token = res.get('nextPageToken')\n",
    "while next_page_token:\n",
    "    res = service.files().list(q = query, pageToken = next_page_token).execute()\n",
    "    files.extend(res.get('files'))\n",
    "    next_page_token = res.get('nextPageToken')\n",
    "\n",
    "files = list(filter(lambda file: file['mimeType'] == 'application/gzip', files))\n",
    "files = sorted(files, key=itemgetter('name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.DataFrame()\n",
    "df_y = pd.DataFrame()\n",
    "df_z = pd.DataFrame()\n",
    "for file in files:\n",
    "    try :\n",
    "        file_object_request = service.files().get_media(fileId = file[\"id\"])\n",
    "        file_bytes = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(file_bytes, file_object_request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        raise ex\n",
    "\n",
    "    filename_to_be_saved = str.lower(\"_\".join([file[\"name\"][1:4], str(int(file[\"name\"][6:8]))]))\n",
    "    file_bytes.seek(0)\n",
    "    with gzip.open(file_bytes, 'rb') as f:\n",
    "        file_content = f.read()\n",
    "    values = list(map(lambda x: x.split(\"\\t\"), file_content.decode().split('\\n')))\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    z_values = []\n",
    "    for value in values:\n",
    "        if(len(value) == 3):\n",
    "            x_values.append(value[0])\n",
    "            y_values.append(value[1])\n",
    "            z_values.append(value[2])\n",
    "\n",
    "    df_x = pd.concat((df_x, pd.DataFrame(data = x_values,columns=[filename_to_be_saved])), axis=1)\n",
    "    df_y = pd.concat((df_y, pd.DataFrame(data = y_values,columns=[filename_to_be_saved])), axis=1)\n",
    "    df_z = pd.concat((df_z, pd.DataFrame(data = z_values,columns=[filename_to_be_saved])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.to_csv(\"x_values.csv\", index=False)\n",
    "df_y.to_csv(\"y_values.csv\", index=False)\n",
    "df_z.to_csv(\"z_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wear_lis_object = service.files().get_media(fileId = os.getenv('file_id'))\n",
    "wear_lis_bytes = io.BytesIO()\n",
    "downloader = MediaIoBaseDownload(wear_lis_bytes, wear_lis_object)\n",
    "done1 = False\n",
    "while done1 is False:\n",
    "    status, done1 = downloader.next_chunk()\n",
    "wear_lis_bytes.seek(0)\n",
    "with gzip.open(wear_lis_bytes, 'rb') as f:\n",
    "    wear_list = f.read().decode()\n",
    "wear_list = list(map(lambda x: x.split(\"\\n\"), wear_list.split(\"\\n\\n\")))\n",
    "wear_list_dict = {}\n",
    "for value in wear_list:\n",
    "    file_name = str.lower(\"_\".join([value[0].split(\" \")[0], value[0].split(\" \")[4]]))\n",
    "    wear_value = value[2].split(\" \")[0]\n",
    "    wear_list_dict[file_name] = wear_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.read_csv(os.getenv('x_Value_file_path'))\n",
    "df_x.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_entropy(adjacency_matrix):\n",
    "    \n",
    "    if adjacency_matrix.shape[0] != adjacency_matrix.shape[1]:\n",
    "        raise Exception(\"Input matrix should be a square matrix\")\n",
    "    \n",
    "    no_of_nodes = adjacency_matrix.shape[0]\n",
    "\n",
    "    normalized_graph_entropy = (1/(no_of_nodes * np.log(no_of_nodes-1))) * (sum(np.log(np.sum(adjacency_matrix, axis=0))))\n",
    "\n",
    "    random_walk_mat = np.zeros(adjacency_matrix.shape)\n",
    "\n",
    "    for i in range(no_of_nodes):\n",
    "        random_walk_mat[i,] = adjacency_matrix[i,]/np.sum(adjacency_matrix[i,])\n",
    "\n",
    "    node_information = np.zeros((1, no_of_nodes))\n",
    "\n",
    "    for i in range(no_of_nodes):\n",
    "        list_values = list(map((lambda j: ((sqrt(random_walk_mat[i, j+1]) - sqrt(random_walk_mat[i, j])) ** 2) if i != j else 0), range(no_of_nodes-1)))\n",
    "        node_information[0, i] = (0.5 * (sum(list_values)))\n",
    "        \n",
    "    normalized_graph_fisher_information = np.sum(node_information[0,], axis=0)/no_of_nodes\n",
    "\n",
    "    return normalized_graph_entropy, normalized_graph_fisher_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df_x.columns\n",
    "df_x_features = []\n",
    "for i in range(len(col_names)):\n",
    "    time_series = df_x.iloc[:, i]\n",
    "    g_natural_vg = NaturalVG()\n",
    "    g_natural_vg.build(time_series)\n",
    "    natural_vg_adjacency = g_natural_vg.as_igraph().get_adjacency()\n",
    "    normalized_graph_entropy, graph_fisher_information = graph_entropy(np.array(natural_vg_adjacency.data))\n",
    "    df_x_features.append([normalized_graph_entropy, graph_fisher_information, col_names[i], wear_list_dict.get(col_names[i], 0)])\n",
    "df_x_features = pd.DataFrame(data = df_x_features, columns = [\"graph_entropy\", \"graph_fisher_information\", \"file_name\", \"wear_value\"])\n",
    "print(df_x_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_features.to_csv(\"x_features.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7976e1f4a752c99aeef7bb5401fcaf626871b3db654ef75e45d29f2c5eaf0939"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
