{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import os\n",
    "import gzip\n",
    "import io\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from operator import itemgetter\n",
    "from ts2vg import NaturalVG\n",
    "from ts2vg import HorizontalVG\n",
    "from math import sqrt\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Google :\n",
    "\n",
    "    def __init__(self, cred_file_path, version, service_name, scopes) -> None:\n",
    "        self.SCOPES = scopes\n",
    "        self.cred_file_path = cred_file_path\n",
    "        self.version = version\n",
    "        self.service_name = service_name\n",
    "\n",
    "    def connect(self) :\n",
    "        \"\"\"Makes a connection to the drive API and returns a service object\n",
    "        \"\"\"\n",
    "        creds = None\n",
    "        # The file token.json stores the user's access and refresh tokens, and is\n",
    "        # created automatically when the authorization flow completes for the first\n",
    "        # time.\n",
    "        if os.path.exists('token.json'):\n",
    "            creds = Credentials.from_authorized_user_file('token.json', self.SCOPES)\n",
    "        # If there are no (valid) credentials available, let the user log in.\n",
    "        if not creds or not creds.valid:\n",
    "            if creds and creds.expired and creds.refresh_token:\n",
    "                creds.refresh(Request())\n",
    "            else:\n",
    "                flow = InstalledAppFlow.from_client_secrets_file(self.cred_file_path, self.SCOPES)\n",
    "                creds = flow.run_local_server(port=0)\n",
    "            # Save the credentials for the next run\n",
    "            with open('token.json', 'w') as token:\n",
    "                token.write(creds.to_json())\n",
    "\n",
    "        try:\n",
    "            service = build(self.service_name, self.version, credentials=creds)\n",
    "            return service\n",
    "\n",
    "        except HttpError as error:\n",
    "            # TODO(developer) - Handle errors from drive API.\n",
    "            print(f'An error occurred: {error}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "connection_class = Google(os.getenv('credentials_file_path'), 'v3', 'drive', ['https://www.googleapis.com/auth/drive'])\n",
    "service = connection_class.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_id = os.getenv('folder_id')\n",
    "query = f\"'{folder_id}' in parents\"\n",
    "\n",
    "res = service.files().list(q = query).execute()\n",
    "files = res.get('files')\n",
    "next_page_token = res.get('nextPageToken')\n",
    "while next_page_token:\n",
    "    res = service.files().list(q = query, pageToken = next_page_token).execute()\n",
    "    files.extend(res.get('files'))\n",
    "    next_page_token = res.get('nextPageToken')\n",
    "\n",
    "files = list(filter(lambda file: file['mimeType'] == 'application/gzip', files))\n",
    "files = sorted(files, key=itemgetter('name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.DataFrame()\n",
    "df_y = pd.DataFrame()\n",
    "df_z = pd.DataFrame()\n",
    "for file in files:\n",
    "    try :\n",
    "        file_object_request = service.files().get_media(fileId = file[\"id\"])\n",
    "        file_bytes = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(file_bytes, file_object_request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        raise ex\n",
    "\n",
    "    filename_to_be_saved = str.lower(\"_\".join([file[\"name\"][1:4], str(int(file[\"name\"][6:8]))]))\n",
    "    file_bytes.seek(0)\n",
    "    with gzip.open(file_bytes, 'rb') as f:\n",
    "        file_content = f.read()\n",
    "    values = list(map(lambda x: x.split(\"\\t\"), file_content.decode().split('\\n')))\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    z_values = []\n",
    "    for value in values:\n",
    "        if(len(value) == 3):\n",
    "            x_values.append(value[0])\n",
    "            y_values.append(value[1])\n",
    "            z_values.append(value[2])\n",
    "\n",
    "    df_x = pd.concat((df_x, pd.DataFrame(data = x_values,columns=[filename_to_be_saved])), axis=1)\n",
    "    df_y = pd.concat((df_y, pd.DataFrame(data = y_values,columns=[filename_to_be_saved])), axis=1)\n",
    "    df_z = pd.concat((df_z, pd.DataFrame(data = z_values,columns=[filename_to_be_saved])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.to_csv(\"x_values.csv\", index=False)\n",
    "df_y.to_csv(\"y_values.csv\", index=False)\n",
    "df_z.to_csv(\"z_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wear_lis_object = service.files().get_media(fileId = os.getenv('file_id'))\n",
    "wear_lis_bytes = io.BytesIO()\n",
    "downloader = MediaIoBaseDownload(wear_lis_bytes, wear_lis_object)\n",
    "done1 = False\n",
    "while done1 is False:\n",
    "    status, done1 = downloader.next_chunk()\n",
    "wear_lis_bytes.seek(0)\n",
    "with gzip.open(wear_lis_bytes, 'rb') as f:\n",
    "    wear_list = f.read().decode()\n",
    "wear_list = list(map(lambda x: x.split(\"\\n\"), wear_list.split(\"\\n\\n\")))\n",
    "wear_list_dict = {}\n",
    "for value in wear_list:\n",
    "    file_name = str.lower(\"_\".join([value[0].split(\" \")[0], value[0].split(\" \")[4]]))\n",
    "    wear_value = value[2].split(\" \")[0]\n",
    "    wear_list_dict[file_name] = wear_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>av1_1</th>\n",
       "      <th>av1_2</th>\n",
       "      <th>av1_3</th>\n",
       "      <th>av1_4</th>\n",
       "      <th>av1_5</th>\n",
       "      <th>av1_6</th>\n",
       "      <th>av1_7</th>\n",
       "      <th>av1_8</th>\n",
       "      <th>av1_9</th>\n",
       "      <th>av1_10</th>\n",
       "      <th>...</th>\n",
       "      <th>cz1_2</th>\n",
       "      <th>cz1_3</th>\n",
       "      <th>cz1_4</th>\n",
       "      <th>cz1_5</th>\n",
       "      <th>cz1_6</th>\n",
       "      <th>cz1_7</th>\n",
       "      <th>cz1_8</th>\n",
       "      <th>cz1_9</th>\n",
       "      <th>cz1_10</th>\n",
       "      <th>cz1_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.210937</td>\n",
       "      <td>0.981445</td>\n",
       "      <td>1.528320</td>\n",
       "      <td>0.615234</td>\n",
       "      <td>1.638184</td>\n",
       "      <td>0.969238</td>\n",
       "      <td>1.132812</td>\n",
       "      <td>1.259766</td>\n",
       "      <td>1.433105</td>\n",
       "      <td>1.276855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>0.766602</td>\n",
       "      <td>0.427246</td>\n",
       "      <td>1.911621</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>1.967773</td>\n",
       "      <td>0.654297</td>\n",
       "      <td>1.367187</td>\n",
       "      <td>2.133789</td>\n",
       "      <td>1.560059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.952148</td>\n",
       "      <td>1.069336</td>\n",
       "      <td>1.467285</td>\n",
       "      <td>0.920410</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>1.308594</td>\n",
       "      <td>1.120605</td>\n",
       "      <td>1.242676</td>\n",
       "      <td>1.398926</td>\n",
       "      <td>1.228027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761719</td>\n",
       "      <td>2.104492</td>\n",
       "      <td>1.748047</td>\n",
       "      <td>0.471191</td>\n",
       "      <td>1.328125</td>\n",
       "      <td>1.069336</td>\n",
       "      <td>1.726074</td>\n",
       "      <td>1.621094</td>\n",
       "      <td>1.188965</td>\n",
       "      <td>1.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.751953</td>\n",
       "      <td>1.330566</td>\n",
       "      <td>0.974121</td>\n",
       "      <td>1.535645</td>\n",
       "      <td>0.651855</td>\n",
       "      <td>1.191406</td>\n",
       "      <td>1.093750</td>\n",
       "      <td>1.254883</td>\n",
       "      <td>1.369629</td>\n",
       "      <td>1.232910</td>\n",
       "      <td>...</td>\n",
       "      <td>1.635742</td>\n",
       "      <td>2.167969</td>\n",
       "      <td>2.155762</td>\n",
       "      <td>0.900879</td>\n",
       "      <td>0.461426</td>\n",
       "      <td>1.577148</td>\n",
       "      <td>2.160645</td>\n",
       "      <td>1.176758</td>\n",
       "      <td>1.838379</td>\n",
       "      <td>1.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693359</td>\n",
       "      <td>1.220703</td>\n",
       "      <td>0.920410</td>\n",
       "      <td>1.103516</td>\n",
       "      <td>1.306152</td>\n",
       "      <td>0.766602</td>\n",
       "      <td>1.140137</td>\n",
       "      <td>1.242676</td>\n",
       "      <td>1.262207</td>\n",
       "      <td>1.218262</td>\n",
       "      <td>...</td>\n",
       "      <td>1.711426</td>\n",
       "      <td>0.747070</td>\n",
       "      <td>0.876465</td>\n",
       "      <td>2.268066</td>\n",
       "      <td>1.765137</td>\n",
       "      <td>2.094727</td>\n",
       "      <td>0.683594</td>\n",
       "      <td>1.804199</td>\n",
       "      <td>1.379395</td>\n",
       "      <td>1.662598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.761719</td>\n",
       "      <td>0.844727</td>\n",
       "      <td>1.313477</td>\n",
       "      <td>0.764160</td>\n",
       "      <td>1.247559</td>\n",
       "      <td>1.208496</td>\n",
       "      <td>1.166992</td>\n",
       "      <td>1.235352</td>\n",
       "      <td>1.396484</td>\n",
       "      <td>1.247559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871582</td>\n",
       "      <td>0.485840</td>\n",
       "      <td>0.544434</td>\n",
       "      <td>1.889648</td>\n",
       "      <td>2.226562</td>\n",
       "      <td>1.352539</td>\n",
       "      <td>1.677246</td>\n",
       "      <td>1.306152</td>\n",
       "      <td>1.511230</td>\n",
       "      <td>1.755371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      av1_1     av1_2     av1_3     av1_4     av1_5     av1_6     av1_7  \\\n",
       "0  1.210937  0.981445  1.528320  0.615234  1.638184  0.969238  1.132812   \n",
       "1  0.952148  1.069336  1.467285  0.920410  0.771484  1.308594  1.120605   \n",
       "2  0.751953  1.330566  0.974121  1.535645  0.651855  1.191406  1.093750   \n",
       "3  0.693359  1.220703  0.920410  1.103516  1.306152  0.766602  1.140137   \n",
       "4  0.761719  0.844727  1.313477  0.764160  1.247559  1.208496  1.166992   \n",
       "\n",
       "      av1_8     av1_9    av1_10  ...     cz1_2     cz1_3     cz1_4     cz1_5  \\\n",
       "0  1.259766  1.433105  1.276855  ...  0.690918  0.766602  0.427246  1.911621   \n",
       "1  1.242676  1.398926  1.228027  ...  0.761719  2.104492  1.748047  0.471191   \n",
       "2  1.254883  1.369629  1.232910  ...  1.635742  2.167969  2.155762  0.900879   \n",
       "3  1.242676  1.262207  1.218262  ...  1.711426  0.747070  0.876465  2.268066   \n",
       "4  1.235352  1.396484  1.247559  ...  0.871582  0.485840  0.544434  1.889648   \n",
       "\n",
       "      cz1_6     cz1_7     cz1_8     cz1_9    cz1_10    cz1_11  \n",
       "0  2.265625  1.967773  0.654297  1.367187  2.133789  1.560059  \n",
       "1  1.328125  1.069336  1.726074  1.621094  1.188965  1.796875  \n",
       "2  0.461426  1.577148  2.160645  1.176758  1.838379  1.718750  \n",
       "3  1.765137  2.094727  0.683594  1.804199  1.379395  1.662598  \n",
       "4  2.226562  1.352539  1.677246  1.306152  1.511230  1.755371  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = pd.read_csv(os.getenv('x_Value_file_path'))\n",
    "df_x.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_entropy(adjacency_matrix):\n",
    "    \n",
    "    if adjacency_matrix.shape[0] != adjacency_matrix.shape[1]:\n",
    "        raise Exception(\"Input matrix should be a square matrix\")\n",
    "    \n",
    "    no_of_nodes = adjacency_matrix.shape[0]\n",
    "\n",
    "    normalized_graph_entropy = (1/(no_of_nodes * np.log(no_of_nodes-1))) * (sum(np.log(np.sum(adjacency_matrix, axis=0))))\n",
    "\n",
    "    random_walk_mat = np.zeros(adjacency_matrix.shape)\n",
    "\n",
    "    for i in range(no_of_nodes):\n",
    "        random_walk_mat[i,] = adjacency_matrix[i,]/np.sum(adjacency_matrix[i,])\n",
    "\n",
    "    node_information = np.zeros((1, no_of_nodes))\n",
    "\n",
    "    for i in range(no_of_nodes):\n",
    "        list_values = list(map((lambda j: ((sqrt(random_walk_mat[i, j+1]) - sqrt(random_walk_mat[i, j])) ** 2) if i != j else 0), range(no_of_nodes-1)))\n",
    "        node_information[0, i] = (0.5 * (sum(list_values)))\n",
    "        \n",
    "    normalized_graph_fisher_information = np.sum(node_information[0,], axis=0)/no_of_nodes\n",
    "\n",
    "    return normalized_graph_entropy, normalized_graph_fisher_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df_x.columns\n",
    "df_x_features = []\n",
    "for i in range(len(col_names)):\n",
    "    time_series = df_x.iloc[:, i]\n",
    "    g_natural_vg = NaturalVG()\n",
    "    g_natural_vg.build(time_series)\n",
    "    natural_vg_adjacency = g_natural_vg.as_igraph().get_adjacency()\n",
    "    normalized_graph_entropy, graph_fisher_information = graph_entropy(np.array(natural_vg_adjacency.data))\n",
    "    df_x_features.append([normalized_graph_entropy, graph_fisher_information, col_names[i], wear_list_dict.get(col_names[i], 0)])\n",
    "df_x_features = pd.DataFrame(data = df_x_features, columns = [\"graph_entropy\", \"graph_fisher_information\", \"file_name\", \"wear_value\"])\n",
    "print(df_x_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_features.to_csv(\"x_features.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7976e1f4a752c99aeef7bb5401fcaf626871b3db654ef75e45d29f2c5eaf0939"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
